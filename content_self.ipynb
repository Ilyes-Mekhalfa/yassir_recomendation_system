{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56ce994",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e0555-3f99-4a7c-9915-f15fc16d91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb #we ll not use xgboost in this version of code we ll use lightgbm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(f\"Available file: {filename}\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774505d",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "users = pd.read_csv(\"/kaggle/input/yassir-ai-market-challenge/yassir_marekt_data_09_2025/yassir_marekt_data_09_2025 2/users_df.csv\")\n",
    "orders = pd.read_csv(\"/kaggle/input/yassir-ai-market-challenge/yassir_marekt_data_09_2025/yassir_marekt_data_09_2025 2/orders_df.csv\")\n",
    "orders_products = pd.read_csv(\"/kaggle/input/yassir-ai-market-challenge/yassir_marekt_data_09_2025/yassir_marekt_data_09_2025 2/orders_products_df.csv\")\n",
    "categories = pd.read_csv(\"/kaggle/input/yassir-ai-market-challenge/yassir_marekt_data_09_2025/yassir_marekt_data_09_2025 2/category_df.csv\")\n",
    "products = pd.read_csv(\"/kaggle/input/yassir-ai-market-challenge/yassir_marekt_data_09_2025/yassir_marekt_data_09_2025 2/products_df.csv\")\n",
    "subcategories = pd.read_csv(\"/kaggle/input/yassir-ai-market-challenge/yassir_marekt_data_09_2025/yassir_marekt_data_09_2025 2/sub_category_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5682fa1",
   "metadata": {},
   "source": [
    "### Merging datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_full = (\n",
    "    products\n",
    "    .merge(categories, on=\"category_id\", how=\"left\")\n",
    "    .merge(subcategories, on=\"sub_category_id\", how=\"left\")\n",
    ")\n",
    "user_product_interactions = orders.merge(\n",
    "    orders_products[['order_id', 'product_id', 'add_to_cart_order', 'reordered']], \n",
    "    on='order_id', \n",
    "    how='inner',\n",
    ")\n",
    "orders = orders.merge(\n",
    "    orders_products[['order_id', 'product_id', 'add_to_cart_order', 'reordered']], \n",
    "    on='order_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82289f74",
   "metadata": {},
   "source": [
    "# Data encoding\n",
    " Manual encoding based on your category list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b701424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_mapping = {\n",
    "    'Hygi√®ne & Beaut√©': 1,\n",
    "    'Poissonnerie': 2,\n",
    "    'CUISINE': 3,\n",
    "    'Caf√©,Th√© & boissons chaudes': 4,\n",
    "    'Fruits et l√©gumes': 5,\n",
    "    'P√¢tes, Riz & C√©r√©ales': 6,\n",
    "    'Huiles, Sauces & Epices': 7,\n",
    "    'Automobile': 8,\n",
    "    'Produits frais & Cr√®merie': 9,\n",
    "    'Soin linge & lessives': 10,\n",
    "    'Biscuits et snacks': 11,\n",
    "    'Yaourt et Desserts': 12,\n",
    "    'Articles scolaires': 13,\n",
    "    'Boissons & Eau': 14,\n",
    "    'Bazar': 15,\n",
    "    '√âpicerie': 16,\n",
    "    'Confiserie & Chocolat': 17,\n",
    "    'Surgel√©s': 18,\n",
    "    'Soin & entretien maison': 19,\n",
    "    'Divers': 20,\n",
    "    'missing': 21\n",
    "}\n",
    "\n",
    "sub_category_mapping = {\n",
    "    'Piles': 1, 'Fruits secs': 2, 'Nettoyant multi-usages': 3, '≈íufs': 4, 'Stylos & Crayons': 5,\n",
    "    'Poissons': 6, 'Cr√®mes Desserts & Mousses': 7, 'Rangement et Organisation': 8, 'Fromages': 9,\n",
    "    'Orientales': 10, 'Chocolat': 11, 'Lben & Ra√Øb': 12, 'Entretien chaussures': 13, 'jardin': 14,\n",
    "    'Yaourts nature & aromatis√©s': 15, 'Capsules': 16, 'COLLAGE ET CISEAU': 17, 'D√©tachants': 18,\n",
    "    'Ardoises': 19, 'Chips & Snacks sal√©s': 20, 'Di√©t√©tiques': 21, 'Chocolats': 22,\n",
    "    'Entretien sanitaire': 23, 'Caf√© en Grains': 24, 'Biscuits': 25, 'Vinaigres & Sauces Salades': 26,\n",
    "    'Frais': 27, 'L√©gumes': 28, 'SAC A DOS ET TROUSSE': 29, 'Couscous': 30, 'Huiles': 31,\n",
    "    'th√© & infusions': 32, 'Riz': 33, 'Hygi√®ne Dentaire': 34, 'Desserts lact√©s & mousses': 35,\n",
    "    'Sucres, farines & semoules': 36, 'D√©odorants & Parfums': 37, 'Parfums & d√©odorants': 38,\n",
    "    'Yaourts': 39, 'Entretien': 40, 'Equipement et D√©co Auto': 41, 'Savon Mains et Solides': 42,\n",
    "    'Equipement de la Cuisine': 43, 'concass√©s.': 44, 'Meubles, cuir & vitres': 45,\n",
    "    'Produits vitres': 46, 'Fruits sec': 47, 'Eau min√©rale': 48, 'desserts √† pr√©parer': 49,\n",
    "    'Produits d√©sinfectants': 50, 'PAPIER': 51, 'Hygi√®ne F√©minine': 52, 'Produits pour p√¢tisserie': 53,\n",
    "    'Bonbons & Confiseries': 54, 'G√©noises & moelleux': 55, 'Pates': 56, 'Sodas': 57,\n",
    "    'Soins Cheveux': 58, 'Cuisson': 59, 'Confiserie orientale': 60, 'Nettoyant sol': 61,\n",
    "    'Fruit': 62, 'Lait': 63, 'Sels & Bases Culinaires': 64, 'C√©r√©ales et L√©gmes Secs': 66,\n",
    "    'Electricit√©': 67, 'Lessives': 68, 'Vaisselle & Accessoires de table': 69, 'Biscuits secs': 70,\n",
    "    'Sels': 71, 'COLORIAGE': 72, 'Snacking': 73, 'Barres & tabelettes de chocolat': 74,\n",
    "    'Barres Chocolat√©es': 75, 'P√¢tes': 76, 'Sauce pour P√¢tes': 77, 'D√©sodorisants & insecticides': 78,\n",
    "    'Hygi√®ne dentaire': 79, 'Bonbons & confiseries': 80, 'Confort maison': 81, 'Legumes': 82,\n",
    "    'Caf√© Moulus': 83, 'produits secs': 84, 'Bonbons': 85, 'Produits secs': 86,\n",
    "    'Cahiers & Registres': 88, 'Sacs a dos': 89, 'Vinaigrettes & Sauces Salades': 90,\n",
    "    'Smen & margarine': 91, 'Ustensile de Cuisine': 93, 'Sauces Froides': 94,\n",
    "    'Bo√Ætes de conservation': 95, 'Adh√©sifs & colles': 96, 'divers': 97, 'Huiles dolives': 98,\n",
    "    'Ustensils de cuisine': 99, 'missing': 100, 'Tablettes de Chocolat': 101, 'Surgel√©s': 102,\n",
    "    'Ustensiles & accessoires': 103, 'Marqueurs': 104, 'Porte documents': 105, 'Compotes': 106,\n",
    "    'Entretien cuisine': 107, 'C√©r√©ales et Barres': 108, 'Snacks': 109, 'Papiers': 110,\n",
    "    'Barres Chocolat√©es & Confiseries': 111, 'Emballage des Aliments': 112, 'Soins du corps': 113,\n",
    "    'Herbes & √âpices': 115, 'Bain & douche': 116, 'Accessoires m√©nagers': 117,\n",
    "    'Shampoings': 119, 'Conserves': 120, 'Jus': 121, 'Creme dessert': 122,\n",
    "    'Caf√©,Th√© & boissons chaudes': 123, 'Fruits': 124, 'Produits d√©graissants': 125,\n",
    "    'Snaking': 126, 'Chips & popcorns': 127, 'Boite de Conservation': 128,\n",
    "    'Cotons & D√©maquillants': 129, 'Eaux': 130, 'Fromage': 131, 'concass√©s': 132,\n",
    "    'Chips': 133, 'Herbes': 134, 'missing': 0\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fedf8",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "adding two columns for diabete and high_blood_presure sicks.\n",
    "this is just a  random generated data with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users['diabete'] = np.random.randint(-1, 1, size=len(users))\n",
    "users['high_blood_presure'] = np.random.randint(0, 1, size=len(users))\n",
    "\n",
    "# Apply the mapping\n",
    "def encode_categories_manual(df):\n",
    "    \"\"\"Encode categories using manual mapping\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    print(df_encoded)\n",
    "    \n",
    "    if 'category' in df_encoded.columns:\n",
    "        df_encoded['category'] = df_encoded['category'].map(category_mapping)\n",
    "        df_encoded['category'] = df_encoded['category'].fillna(21)\n",
    "        \n",
    "    if 'sub_category' in df_encoded.columns:\n",
    "        df_encoded['sub_category'] = df_encoded['sub_category'].map(sub_category_mapping).fillna(0).astype(int)\n",
    "        df_encoded['sub_category'] = df_encoded['sub_category'].fillna(0)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "encoded_data = encode_categories_manual(products_full)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f6d98",
   "metadata": {},
   "source": [
    "#### creating simplea training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_training_data():\n",
    "    \"\"\"Create a simpler training dataset\"\"\"\n",
    "    print(\"2. Creating training data...\")\n",
    "    \n",
    "    user_purchase_counts = orders['user_id'].value_counts()\n",
    "    active_users = user_purchase_counts[user_purchase_counts >= 2].index\n",
    "    \n",
    "    print(f\"Active users (‚â•2 purchases): {len(active_users)}\")\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    for user_id in active_users[:500]:\n",
    "        user_orders = orders[orders['user_id'] == user_id]\n",
    "        purchased_products = set(user_orders['product_id'].values)\n",
    "        \n",
    "        for product_id in purchased_products:\n",
    "            training_data.append({\n",
    "                'user_id': user_id,\n",
    "                'product_id': product_id,\n",
    "                'purchased': 1\n",
    "            })\n",
    "        \n",
    "        all_products = set(products_full['product_id'])\n",
    "        non_purchased = list(all_products - purchased_products)\n",
    "        \n",
    "        n_negative = min(len(purchased_products) * 2, len(non_purchased))\n",
    "        if n_negative > 0:\n",
    "            negative_samples = np.random.choice(non_purchased, n_negative, replace=False)\n",
    "            for product_id in negative_samples:\n",
    "                training_data.append({\n",
    "                    'user_id': user_id,\n",
    "                    'product_id': product_id,\n",
    "                    'purchased': 0\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(training_data)\n",
    "    df['category'] = encoded_data['category']\n",
    "    df['sub_category'] = encoded_data['sub_category']\n",
    "    print(f\"Training dataset: {len(df)} samples\")\n",
    "    print(f\"Positive samples: {df['purchased'].sum()}\")\n",
    "    print(f\"Negative samples: {len(df) - df['purchased'].sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "binary_data = create_simple_training_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8b292",
   "metadata": {},
   "source": [
    "# Training the model using lightGBM model\n",
    "this model is lighter then using xgboost for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_features(binary_data):\n",
    "    \"\"\"Create features for training\"\"\"\n",
    "    print(\"Creating features for training data...\")\n",
    "    \n",
    "    user_features = user_product_interactions.groupby('user_id').agg({\n",
    "        'product_id': ['count', 'nunique'],\n",
    "        'order_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    user_features.columns = ['user_id', 'user_total_purchases', 'user_unique_products', 'user_total_orders']\n",
    "    \n",
    "    product_features = user_product_interactions.groupby('product_id').agg({\n",
    "        'user_id': ['count', 'nunique'],\n",
    "        'order_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    product_features.columns = ['product_id', 'product_total_purchases', 'product_unique_users', 'product_total_orders']\n",
    "    \n",
    "    df_featured = binary_data.merge(user_features, on='user_id', how='left')\n",
    "    df_featured = df_featured.merge(product_features, on='product_id', how='left')\n",
    "    df_featured = df_featured.merge(binary_data[['product_id', 'category', 'sub_category']], on='product_id', how='left')\n",
    "    \n",
    "    numeric_cols = ['user_total_purchases', 'user_unique_products', 'user_total_orders',\n",
    "                   'product_total_purchases', 'product_unique_users', 'product_total_orders']\n",
    "    df_featured[numeric_cols] = df_featured[numeric_cols].fillna(0)\n",
    "    \n",
    "    print(f\"‚úÖ Features created. Final data shape: {df_featured.shape}\")\n",
    "    print(f\"Columns: {df_featured.columns.tolist()}\")\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "featured_data = create_training_features(binary_data)\n",
    "\n",
    "\n",
    "\n",
    "def train_with_map20_evaluation(featured_data):\n",
    "    \"\"\"Train model with integrated MAP@20 evaluation\"\"\"\n",
    "    \n",
    "    feature_columns = [col for col in featured_data.columns if col not in ['user_id', 'product_id', 'purchased']]\n",
    "    \n",
    "    X = featured_data[feature_columns]\n",
    "    y = featured_data['purchased']\n",
    "    groups = featured_data['user_id']\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=3)\n",
    "    \n",
    "    map_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(group_kfold.split(X, y, groups)):\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        groups_train, groups_val = groups.iloc[train_idx], groups.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=200, #reduced to 200 to train faster the model\n",
    "            max_depth=6, # to stop in depth 6 to avoid overfitting if im not wrong like a classifier!! ... im not sure about these last words\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42 + fold,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[lgb.early_stopping(20, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        fold_map = calculate_map20(model, X_val, y_val, groups_val)\n",
    "        map_scores.append(fold_map)\n",
    "        models.append(model)\n",
    "    \n",
    "    print(f\"MAP@20 scores: {[f'{score:.4f}' for score in map_scores]}\")\n",
    "    print(f\"Mean MAP@20: {np.mean(map_scores):.4f} (+/- {np.std(map_scores):.4f})\")\n",
    "    \n",
    "    avg_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "    print(f\"\\nTraining final model with {avg_best_iteration} iterations...\")\n",
    "    \n",
    "    final_model = lgb.LGBMClassifier(\n",
    "        n_estimators=avg_best_iteration,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X, y)\n",
    "    \n",
    "    final_map = calculate_map20(final_model, X, y, groups)\n",
    "    print(f\"Final training MAP@20: {final_map:.4f}\")\n",
    "    \n",
    "    return final_model, feature_columns, np.mean(map_scores)\n",
    "\n",
    "def calculate_map20(model, X, y, user_groups):\n",
    "    \"\"\"Calculate MAP@20 for given predictions\"\"\"\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'user_id': user_groups.values,\n",
    "        'true_label': y.values,\n",
    "        'pred_score': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    map_scores = []\n",
    "    \n",
    "    for user_id in results_df['user_id'].unique():\n",
    "        user_data = results_df[results_df['user_id'] == user_id]\n",
    "        user_data = user_data.sort_values('pred_score', ascending=False).head(20)\n",
    "        relevant_items = user_data['true_label'].values\n",
    "        ap = calculate_average_precision(relevant_items)\n",
    "        map_scores.append(ap)\n",
    "    \n",
    "    return np.mean(map_scores)\n",
    "\n",
    "def calculate_average_precision(relevant_items, k=20):\n",
    "    \"\"\"Calculate Average Precision @ K\"\"\"\n",
    "    if np.sum(relevant_items) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, rel in enumerate(relevant_items[:k]):\n",
    "        if rel == 1:\n",
    "            num_hits += 1.0\n",
    "            precision_at_i = num_hits / (i + 1.0)\n",
    "            score += precision_at_i\n",
    "    \n",
    "    return score / min(np.sum(relevant_items), k)\n",
    "\n",
    "print(\"Starting MAP@20 optimized training...\")\n",
    "model, feature_columns, cv_map_score = train_with_map20_evaluation(featured_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776cca01",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. USER PURCHASE DISTRIBUTION\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "user_purchase_counts = orders.groupby('user_id').size()\n",
    "plt.hist(user_purchase_counts, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Purchases per User', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Users', fontsize=12, fontweight='bold')\n",
    "plt.title('Distribution of User Purchase Frequency', fontsize=14, fontweight='bold')\n",
    "plt.axvline(user_purchase_counts.mean(), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {user_purchase_counts.mean():.1f}')\n",
    "plt.legend()\n",
    "\n",
    "# 2. TOP 20 MOST POPULAR PRODUCTS\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "top_products = orders_products['product_id'].value_counts().head(20)\n",
    "plt.barh(range(len(top_products)), top_products.values, color='coral')\n",
    "plt.yticks(range(len(top_products)), [f'Product {pid}' for pid in top_products.index], fontsize=9)\n",
    "plt.xlabel('Purchase Count', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Most Purchased Products', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# 3. CATEGORY DISTRIBUTION\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "category_counts = encoded_data['category'].value_counts().head(10)\n",
    "colors = sns.color_palette(\"husl\", len(category_counts))\n",
    "plt.pie(category_counts.values, labels=[f'Cat {c}' for c in category_counts.index], \n",
    "        autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "plt.title('Top 10 Product Categories Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. REORDER RATE ANALYSIS\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "reorder_stats = orders_products.groupby('product_id')['reordered'].agg(['sum', 'count'])\n",
    "reorder_stats['reorder_rate'] = (reorder_stats['sum'] / reorder_stats['count']) * 100\n",
    "reorder_rate_dist = reorder_stats['reorder_rate']\n",
    "plt.hist(reorder_rate_dist, bins=30, color='green', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Reorder Rate (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Products', fontsize=12, fontweight='bold')\n",
    "plt.title('Product Reorder Rate Distribution', fontsize=14, fontweight='bold')\n",
    "plt.axvline(reorder_rate_dist.mean(), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {reorder_rate_dist.mean():.1f}%')\n",
    "plt.legend()\n",
    "\n",
    "# 5. ORDERS PER USER BOXPLOT\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "orders_per_user = orders.groupby('user_id')['order_id'].nunique()\n",
    "bp = plt.boxplot(orders_per_user, vert=True, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "bp['boxes'][0].set_alpha(0.7)\n",
    "plt.ylabel('Number of Orders', fontsize=12, fontweight='bold')\n",
    "plt.title('Orders per User Distribution (Boxplot)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. PRODUCTS PER ORDER\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "products_per_order = orders_products.groupby('order_id').size()\n",
    "plt.hist(products_per_order, bins=50, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Products per Order', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Orders', fontsize=12, fontweight='bold')\n",
    "plt.title('Distribution of Products per Order', fontsize=14, fontweight='bold')\n",
    "plt.axvline(products_per_order.mean(), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {products_per_order.mean():.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('yassir_market_analysis_part1.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SECOND FIGURE - MODEL & FEATURE ANALYSIS\n",
    "fig2 = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# 7. TRAINING DATA BALANCE\n",
    "ax7 = plt.subplot(2, 3, 1)\n",
    "class_dist = binary_data['purchased'].value_counts()\n",
    "colors_class = ['#ff6b6b', '#4ecdc4']\n",
    "bars = plt.bar(['Not Purchased', 'Purchased'], class_dist.values, color=colors_class, alpha=0.8, edgecolor='black')\n",
    "plt.ylabel('Count', fontsize=12, fontweight='bold')\n",
    "plt.title('Training Data Class Balance', fontsize=14, fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}\\n({height/class_dist.sum()*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 8. USER FEATURES CORRELATION\n",
    "ax8 = plt.subplot(2, 3, 2)\n",
    "feature_cols = ['user_total_purchases', 'user_unique_products', 'user_total_orders']\n",
    "if all(col in featured_data.columns for col in feature_cols):\n",
    "    corr_matrix = featured_data[feature_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('User Features Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Feature data not available', ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "\n",
    "# 9. PRODUCT POPULARITY DISTRIBUTION\n",
    "ax9 = plt.subplot(2, 3, 3)\n",
    "product_popularity = orders_products.groupby('product_id').size()\n",
    "plt.hist(np.log10(product_popularity + 1), bins=40, color='orange', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Log10(Purchase Count + 1)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Products', fontsize=12, fontweight='bold')\n",
    "plt.title('Product Popularity Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 10. SUBCATEGORY DISTRIBUTION\n",
    "ax10 = plt.subplot(2, 3, 4)\n",
    "top_subcats = encoded_data['sub_category'].value_counts().head(15)\n",
    "plt.barh(range(len(top_subcats)), top_subcats.values, color=sns.color_palette(\"viridis\", len(top_subcats)))\n",
    "plt.yticks(range(len(top_subcats)), [f'SubCat {sc}' for sc in top_subcats.index], fontsize=9)\n",
    "plt.xlabel('Product Count', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 15 Sub-Categories', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# 11. USER ENGAGEMENT SCATTER\n",
    "ax11 = plt.subplot(2, 3, 5)\n",
    "user_stats = orders.groupby('user_id').agg({\n",
    "    'order_id': 'nunique',\n",
    "    'product_id': 'count'\n",
    "}).head(1000)\n",
    "plt.scatter(user_stats['order_id'], user_stats['product_id'], \n",
    "            alpha=0.5, s=30, c='teal', edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Number of Orders', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Total Products Purchased', fontsize=12, fontweight='bold')\n",
    "plt.title('User Engagement: Orders vs Products', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 12. ADD TO CART ORDER DISTRIBUTION\n",
    "ax12 = plt.subplot(2, 3, 6)\n",
    "cart_order_dist = orders_products['add_to_cart_order'].value_counts().sort_index().head(20)\n",
    "plt.plot(cart_order_dist.index, cart_order_dist.values, marker='o', \n",
    "         linewidth=2, markersize=8, color='darkblue', label='Frequency')\n",
    "plt.fill_between(cart_order_dist.index, cart_order_dist.values, alpha=0.3, color='lightblue')\n",
    "plt.xlabel('Add to Cart Order', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.title('Add to Cart Order Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('yassir_market_analysis_part2.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ All graphs generated successfully!\")\n",
    "print(\"üìä Saved files: yassir_market_analysis_part1.png, yassir_market_analysis_part2.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471b544",
   "metadata": {},
   "source": [
    "# Subbmition section this is for the Yassir competition in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c678a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "test_users_df = pd.read_csv('/kaggle/input/yassir-ai-market-challenge/test_new_version.csv')\n",
    "test_users_df = test_users_df.sample(frac=0.2, random_state=42)\n",
    "target_users = test_users_df['user_id'].values\n",
    "\n",
    "print(f\"Generating recommendations for {len(target_users):,} users...\")\n",
    "\n",
    "# Prepare features\n",
    "all_user_features = user_product_interactions.groupby('user_id').agg({\n",
    "    'product_id': ['count', 'nunique'],\n",
    "    'order_id': 'nunique'\n",
    "}).reset_index()\n",
    "all_user_features.columns = ['user_id', 'user_total_purchases', 'user_unique_products', 'user_total_orders']\n",
    "\n",
    "# Pre-compute popular products\n",
    "popular_products = featured_data.nlargest(20, 'product_total_purchases')['product_id'].values\n",
    "popular_rec_str = \" \".join(map(str, popular_products))\n",
    "\n",
    "# Pre-compute user purchases (MUCH FASTER - OPTIMIZED)\n",
    "print(\"Pre-computing user purchase history...\")\n",
    "user_purchases_dict = user_product_interactions.groupby('user_id')['product_id'].apply(set).to_dict()\n",
    "print(f\"Cached purchase history for {len(user_purchases_dict):,} users\")\n",
    "\n",
    "all_products = products_full['product_id'].values\n",
    "submission_rows = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, user_id in enumerate(target_users):\n",
    "    # More frequent progress updates\n",
    "    if i % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = i / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(target_users) - i) / rate if rate > 0 else 0\n",
    "        print(f\"  Progress: {i}/{len(target_users)} ({i/len(target_users)*100:.1f}%) - {rate:.1f} users/sec - ETA: {eta/60:.1f} min\")\n",
    "    \n",
    "    # Check if user exists (FAST lookup)\n",
    "    user_exists = user_id in user_purchases_dict\n",
    "    \n",
    "    if not user_exists:\n",
    "        submission_rows.append([user_id, popular_rec_str])\n",
    "        continue\n",
    "    \n",
    "    # Get cached purchases\n",
    "    user_purchased = user_purchases_dict[user_id]\n",
    "    \n",
    "    # CRITICAL: Limit candidates BEFORE creating DataFrame\n",
    "    candidates = [p for p in all_products if p not in user_purchased]\n",
    "    \n",
    "    if len(candidates) == 0 or len(candidates) < 20:\n",
    "        submission_rows.append([user_id, popular_rec_str])\n",
    "        continue\n",
    "    \n",
    "    # LIMIT to 50 candidates for speed\n",
    "    if len(candidates) > 25:\n",
    "        candidates = np.random.choice(candidates, 25, replace=False)\n",
    "    \n",
    "    # Create prediction data\n",
    "    pred_data = pd.DataFrame({\n",
    "        'user_id': [user_id] * len(candidates),\n",
    "        'product_id': candidates\n",
    "    })\n",
    "    \n",
    "    # Merge features\n",
    "    pred_data = pred_data.merge(all_user_features, on='user_id', how='left')\n",
    "    pred_data = pred_data.merge(featured_data, on='product_id', how='left')\n",
    "    pred_data = pred_data.fillna(0)\n",
    "    \n",
    "    # Add missing columns\n",
    "    for col in feature_columns:\n",
    "        if col not in pred_data.columns:\n",
    "            pred_data[col] = 0\n",
    "    \n",
    "    # Predict\n",
    "    X_pred = pred_data[feature_columns]\n",
    "    scores = model.predict_proba(X_pred)[:, 1]\n",
    "    \n",
    "    # Get top 20\n",
    "    top_indices = np.argsort(scores)[-10:][::-1]\n",
    "    recommendations = pred_data.iloc[top_indices]['product_id'].values\n",
    "    rec_str = \" \".join(map(str, recommendations))\n",
    "    submission_rows.append([user_id, rec_str])\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Processed {len(submission_rows):,} users in {elapsed_total/60:.1f} minutes\")\n",
    "\n",
    "# Save submission\n",
    "submission_df = pd.DataFrame(submission_rows, columns=['user_id', 'product_id'])\n",
    "submission_path = \"submission.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ SUBMISSION COMPLETE!\")\n",
    "print(f\"\\n üìÅ File: {submission_path}\")\n",
    "print(f\"\\n üìà Rows: {len(submission_df):,}\")\n",
    "print(f\"\\n üìä Unique users: {submission_df['user_id'].nunique():,}\")\n",
    "print(\"\\n PIPELINE COMPLETED SUCCESSFULLY!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
